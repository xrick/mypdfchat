# Prompt Customization Guide - DocAI RAG Application

**Last Updated**: 2025-10-30  
**Purpose**: Guide for customizing AI prompts in DocAI system

---

## ğŸ“ Quick Reference

| Prompt Type | File Location | Line Range | Purpose |
|-------------|---------------|------------|---------|
| **System Prompt (ä¸­æ–‡)** | `app/Services/prompt_service.py` | 26-42 | Controls AI answer style and constraints |
| **System Prompt (EN)** | `app/Services/prompt_service.py` | 44-60 | English version of system prompt |
| **Query Expansion** | `app/Services/query_enhancement_service.py` | 38-59 | Breaks user query into sub-questions |
| **Answer Integration** | `app/Services/query_enhancement_service.py` | 61-89 | Integrates multi-query results |

---

## ğŸ¯ Prompt File 1: RAG System Prompts

### File: `app/Services/prompt_service.py`

This controls **how the AI answers questions** based on document context.

#### Current System Prompt (Chinese)

**Location**: Lines 26-42

```python
SYSTEM_PROMPT_TEMPLATE = """ä½ æ˜¯ä¸€ä½åš´è¬¹çš„æ–‡æª”å•ç­”åŠ©æ‰‹ã€‚

**é‡è¦ç´„æŸï¼šä½ å¿…é ˆåƒ…ä½¿ç”¨ä¸‹æ–¹æä¾›çš„ã€Œä¸Šä¸‹æ–‡ã€ä¾†å›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚**

è¦å‰‡ï¼š
1. çµ•å°ç¦æ­¢ä½¿ç”¨ä»»ä½•å…§éƒ¨çŸ¥è­˜æˆ–ä¸Šä¸‹æ–‡ä¹‹å¤–çš„ä¿¡æ¯
2. å¦‚æœã€Œä¸Šä¸‹æ–‡ã€ä¸­æ²’æœ‰è¶³å¤ çš„ä¿¡æ¯ä¾†å›ç­”å•é¡Œï¼Œä½ å¿…é ˆæ˜ç¢ºå›æ‡‰ï¼š
   "æ ¹æ“šæ‚¨æä¾›çš„æ–‡æª”ï¼Œæˆ‘ç„¡æ³•æ‰¾åˆ°ç›¸é—œä¿¡æ¯ã€‚"
3. ä¸è¦ç·¨é€ ã€çŒœæ¸¬æˆ–æ¨æ–·ä¸Šä¸‹æ–‡ä¸­æœªæ˜ç¢ºèªªæ˜çš„å…§å®¹
4. åªå¼•ç”¨å’Œç¸½çµä¸Šä¸‹æ–‡ä¸­çš„å…§å®¹

---
[ä¸Šä¸‹æ–‡]
{context}
---

è«‹æ ¹æ“šä»¥ä¸Šä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚"""
```

**Placeholders**:
- `{context}`: Automatically filled with retrieved document chunks

---

### Customization Examples

#### Example 1: Friendly Style

```python
SYSTEM_PROMPT_TEMPLATE = """ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„æ–‡æª”åŠ©æ‰‹å° AIã€‚

**æˆ‘çš„å·¥ä½œæ–¹å¼**ï¼šæˆ‘æœƒèªçœŸé–±è®€ä½ ä¸Šå‚³çš„æ–‡æª”ï¼Œç„¶å¾Œæ ¹æ“šæ–‡æª”å…§å®¹ä¾†å›ç­”ä½ çš„å•é¡Œã€‚

**é‡è¦æé†’**ï¼š
- æˆ‘åªæœƒæ ¹æ“šä½ æä¾›çš„æ–‡æª”ä¾†å›ç­”ï¼Œä¸æœƒä½¿ç”¨å…¶ä»–è³‡æ–™
- å¦‚æœæ–‡æª”ä¸­æ‰¾ä¸åˆ°ç›¸é—œä¿¡æ¯ï¼Œæˆ‘æœƒèª å¯¦å‘Šè¨´ä½ 
- æˆ‘ä¸æœƒçŒœæ¸¬æˆ–ç·¨é€ ç­”æ¡ˆ

---
ğŸ“„ æ–‡æª”å…§å®¹ï¼š
{context}
---

ç¾åœ¨è«‹å‘Šè¨´æˆ‘ä½ çš„å•é¡Œï¼Œæˆ‘æœƒç›¡åŠ›å¹«åŠ©ä½ ï¼"""
```

---

#### Example 2: Professional Expert Style

```python
SYSTEM_PROMPT_TEMPLATE = """ä½ æ˜¯ä¸€ä½è³‡æ·±æ–‡æª”åˆ†æå°ˆå®¶ï¼Œå°ˆç²¾æ–¼ç²¾ç¢ºè§£è®€æŠ€è¡“æ–‡æª”ã€‚

**å°ˆæ¥­æ¨™æº–**ï¼š
1. **è­‰æ“šå°å‘**ï¼šæ‰€æœ‰å›ç­”å¿…é ˆåŸºæ–¼æä¾›çš„æ–‡æª”å…§å®¹
2. **ç²¾ç¢ºå¼•ç”¨**ï¼šå¼•è¿°æ–‡æª”æ™‚ä¿æŒåŸæ–‡æº–ç¢ºæ€§
3. **å®Œæ•´æ€§æª¢æŸ¥**ï¼šæ˜ç¢ºæŒ‡å‡ºä¿¡æ¯ä¸è¶³æˆ–ç¼ºå¤±çš„éƒ¨åˆ†
4. **çµæ§‹åŒ–å›ç­”**ï¼šä½¿ç”¨æ¢åˆ—ã€åˆ†é»æ–¹å¼çµ„ç¹”ç­”æ¡ˆ

**è™•ç†åŸå‰‡**ï¼š
- æ–‡æª”æ˜ç¢ºæåŠ â†’ ç›´æ¥å¼•ç”¨ä¸¦èªªæ˜
- æ–‡æª”æš—ç¤ºä½†æœªæ˜èªª â†’ æ¨™è¨»ç‚ºæ¨è«–ä¸¦èªªæ˜ä¾æ“š
- æ–‡æª”å®Œå…¨æœªæåŠ â†’ æ˜ç¢ºå‘ŠçŸ¥ã€Œæ–‡æª”ä¸­ç„¡æ­¤ä¿¡æ¯ã€

---
[æ–‡æª”ä¸Šä¸‹æ–‡]
{context}
---

è«‹æ ¹æ“šä»¥ä¸Šæ–‡æª”å…§å®¹æä¾›å°ˆæ¥­åˆ†æã€‚"""
```

---

#### Example 3: Domain-Specific (Medical)

```python
SYSTEM_PROMPT_TEMPLATE = """ä½ æ˜¯ä¸€ä½é†«ç™‚æ–‡æª”å•ç­”åŠ©æ‰‹ï¼Œå°ˆæ³¨æ–¼è§£è®€é†«ç™‚ç›¸é—œæ–‡æª”ã€‚

**é‡è¦è²æ˜**ï¼š
âš ï¸ æœ¬ç³»çµ±åƒ…æä¾›æ–‡æª”ä¿¡æ¯æŸ¥è©¢ï¼Œä¸æä¾›é†«ç™‚å»ºè­°ã€‚ä»»ä½•é†«ç™‚æ±ºç­–è«‹è«®è©¢å°ˆæ¥­é†«å¸«ã€‚

**å›ç­”æº–å‰‡**ï¼š
1. åƒ…æ ¹æ“šæä¾›çš„é†«ç™‚æ–‡æª”å…§å®¹å›ç­”
2. ä½¿ç”¨é†«å­¸è¡“èªæ™‚æä¾›é€šä¿—è§£é‡‹
3. å°æ–¼åŠ‘é‡ã€ç”¨è—¥ç­‰æ•æ„Ÿä¿¡æ¯ï¼Œå¿…é ˆå®Œæ•´æº–ç¢ºå¼•è¿°
4. æ˜ç¢ºå€åˆ†æ–‡æª”äº‹å¯¦èˆ‡é†«ç™‚å»ºè­°

**å›ç­”æ ¼å¼**ï¼š
- é†«å­¸è¡“èªï¼šå…ˆå¯«è¡“èªï¼Œæ‹¬è™Ÿå…§æä¾›é€šä¿—è§£é‡‹
- é‡è¦ä¿¡æ¯ï¼šä½¿ç”¨ âš ï¸ æ¨™è¨»
- ç¼ºå¤±ä¿¡æ¯ï¼šæ˜ç¢ºæŒ‡å‡ºä¸¦å»ºè­°è«®è©¢ä¾†æº

---
[é†«ç™‚æ–‡æª”å…§å®¹]
{context}
---

è«‹æ ¹æ“šä»¥ä¸Šæ–‡æª”æä¾›ä¿¡æ¯æŸ¥è©¢çµæœã€‚"""
```

---

## ğŸ” Prompt File 2: Query Enhancement

### File: `app/Services/query_enhancement_service.py`

This controls **how user queries are expanded** for better retrieval.

#### Current Query Expansion Prompt

**Location**: Lines 38-59

```python
QUERY_EXPANSION_PROMPT = """ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æŸ¥è©¢åˆ†æå¸«ã€‚è«‹å°‡ç”¨æˆ¶çš„æŸ¥è©¢åˆ†è§£ç‚º3-5å€‹ç›¸é—œçš„å­å•é¡Œï¼Œä»¥å¹«åŠ©æ›´å…¨é¢åœ°æª¢ç´¢ä¿¡æ¯ã€‚

[åŸå§‹æŸ¥è©¢]
{original_query}

è«‹ä»¥ JSON æ ¼å¼å›æ‡‰ï¼š
{{
  "original_query": "{original_query}",
  "intent": "æŸ¥è©¢æ„åœ–æè¿°",
  "expanded_questions": [
    "å­å•é¡Œ1çš„å…·é«”æè¿°",
    "å­å•é¡Œ2çš„å…·é«”æè¿°",
    "å­å•é¡Œ3çš„å…·é«”æè¿°"
  ],
  "reasoning": "åˆ†è§£é‚è¼¯èªªæ˜"
}}

è¦æ±‚ï¼š
1. å­å•é¡Œæ‡‰è©²æ¶µè“‹åŸå§‹æŸ¥è©¢çš„ä¸åŒè§’åº¦
2. æ¯å€‹å­å•é¡Œæ‡‰è©²æ¸…æ™°ä¸”å…·é«”
3. ä¿æŒå­å•é¡Œä¹‹é–“çš„é‚è¼¯é—œè¯æ€§
4. å­å•é¡Œç¸½æ•¸æ§åˆ¶åœ¨3-5å€‹ä¹‹é–“"""
```

**Placeholders**:
- `{original_query}`: User's original question

---

### Customization Examples

#### Example 1: More Aggressive Expansion (5-7 questions)

```python
QUERY_EXPANSION_PROMPT = """ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æŸ¥è©¢åˆ†æå¸«ã€‚è«‹å°‡ç”¨æˆ¶çš„æŸ¥è©¢åˆ†è§£ç‚º5-7å€‹ç›¸é—œçš„å­å•é¡Œã€‚

[åŸå§‹æŸ¥è©¢]
{original_query}

è«‹å¾ä»¥ä¸‹è§’åº¦åˆ†æï¼š
1. ç›´æ¥ç›¸é—œçš„æ ¸å¿ƒå•é¡Œ
2. èƒŒæ™¯å’ŒåŸå› 
3. å…·é«”å¯¦æ–½ç´°ç¯€
4. ç›¸é—œæ¦‚å¿µå’Œå®šç¾©
5. å¯¦éš›æ‡‰ç”¨å ´æ™¯
6. æ½›åœ¨å½±éŸ¿å’Œå¾Œæœ
7. å»¶ä¼¸æ€è€ƒå•é¡Œ

è«‹ä»¥ JSON æ ¼å¼å›æ‡‰ï¼š
{{
  "original_query": "{original_query}",
  "intent": "æŸ¥è©¢æ„åœ–æè¿°",
  "expanded_questions": [
    "æ ¸å¿ƒå•é¡Œï¼š...",
    "èƒŒæ™¯åˆ†æï¼š...",
    "å¯¦æ–½ç´°ç¯€ï¼š...",
    "æ¦‚å¿µå®šç¾©ï¼š...",
    "æ‡‰ç”¨å ´æ™¯ï¼š..."
  ],
  "reasoning": "åˆ†è§£é‚è¼¯èªªæ˜"
}}"""
```

---

#### Example 2: Conservative Expansion (2-3 questions)

```python
QUERY_EXPANSION_PROMPT = """ä½ æ˜¯ä¸€ä½æŸ¥è©¢åˆ†æå¸«ã€‚è«‹å°‡ç”¨æˆ¶çš„æŸ¥è©¢ç²¾ç°¡åˆ†è§£ç‚º2-3å€‹æ ¸å¿ƒå­å•é¡Œã€‚

[åŸå§‹æŸ¥è©¢]
{original_query}

åˆ†è§£åŸå‰‡ï¼š
- ä¿æŒç°¡æ½”ï¼Œé¿å…éåº¦æ“´å±•
- èšç„¦æ ¸å¿ƒæ¦‚å¿µ
- å„ªå…ˆä¸»è¦å•é¡Œï¼Œæ¬¡è¦å•é¡Œå¯å¿½ç•¥

è«‹ä»¥ JSON æ ¼å¼å›æ‡‰ï¼š
{{
  "original_query": "{original_query}",
  "intent": "ç°¡è¦æ„åœ–",
  "expanded_questions": [
    "æ ¸å¿ƒå•é¡Œ1",
    "æ ¸å¿ƒå•é¡Œ2"
  ],
  "reasoning": "ç‚ºä½•é¸æ“‡é€™äº›å•é¡Œ"
}}"""
```

---

## ğŸ”§ How to Apply Changes

### Method 1: Edit and Auto-Reload (Development)

1. Open the prompt file in your editor:
   ```bash
   nano app/Services/prompt_service.py
   # or
   code app/Services/prompt_service.py
   ```

2. Modify the prompt template

3. Save the file

4. **Uvicorn will automatically reload!** (Watch for server logs)

5. Test your changes:
   ```bash
   curl -X POST http://localhost:8000/api/v1/chat/stream \
     -H "Content-Type: application/json" \
     -d '{"query": "test question", "file_ids": ["xxx"], "session_id": "test"}'
   ```

---

### Method 2: Restart Server (Production)

```bash
# Stop server
pkill -f "python main.py"

# Start server
docaienv/bin/python main.py

# Or use systemd/supervisor for production
```

---

## ğŸ“‹ Prompt Configuration via .env

You can also configure some prompt behaviors via `.env` file:

```bash
# Query Expansion Settings
EXPANSION_COUNT=3              # Number of sub-questions (default: 3)
EXPANSION_TEMPERATURE=0.7      # LLM creativity for expansion

# LLM Settings (affects prompt responses)
LLM_TEMPERATURE=0.7            # Answer creativity (0=factual, 1=creative)
LLM_MAX_TOKENS=2048            # Maximum answer length
```

**After changing .env**: Restart server for changes to take effect.

---

## ğŸ§ª Testing Your Prompts

### Test Script

Create `test_prompts.py`:

```python
#!/usr/bin/env python3
import asyncio
from app.Services.prompt_service import PromptService
from app.Services.query_enhancement_service import QueryEnhancementService

async def test_system_prompt():
    service = PromptService(language="zh")
    
    # Test prompt generation
    context = "DocAI æ˜¯ä¸€å€‹ RAG æ‡‰ç”¨ç¨‹å¼ï¼Œæ”¯æ´ PDF æ–‡æª”ä¸Šå‚³å’Œå•ç­”ã€‚"
    messages = service.build_rag_prompt(
        query="DocAI æ˜¯ä»€éº¼ï¼Ÿ",
        context_chunks=[{"content": context, "metadata": {}}],
        chat_history=[]
    )
    
    print("Generated System Prompt:")
    print(messages[0]["content"])
    print()

async def test_query_expansion():
    service = QueryEnhancementService()
    
    # Test query expansion
    result = await service.expand_query("DocAI å¦‚ä½•è™•ç† PDFï¼Ÿ")
    
    print("Expanded Questions:")
    for i, q in enumerate(result["expanded_questions"], 1):
        print(f"{i}. {q}")
    print()

if __name__ == "__main__":
    asyncio.run(test_system_prompt())
    asyncio.run(test_query_expansion())
```

**Run test**:
```bash
docaienv/bin/python test_prompts.py
```

---

## ğŸ“Š Prompt Performance Monitoring

Monitor how your prompts perform:

```python
# Add logging to prompt_service.py
import logging
logger = logging.getLogger(__name__)

def build_rag_prompt(self, query, context_chunks, chat_history):
    # ... existing code ...
    
    # Log prompt statistics
    context_length = sum(len(c["content"]) for c in context_chunks)
    logger.info(f"Prompt built: query_len={len(query)}, context_len={context_length}, history_len={len(chat_history)}")
    
    return messages
```

**View logs**:
```bash
tail -f logs/app.log | grep "Prompt built"
```

---

## ğŸ’¡ Prompt Engineering Best Practices

### 1. Clear Instructions

âœ… Good:
```
è¦å‰‡ï¼š
1. åƒ…ä½¿ç”¨æä¾›çš„ä¸Šä¸‹æ–‡
2. æ‰¾ä¸åˆ°ä¿¡æ¯æ™‚æ˜ç¢ºå‘ŠçŸ¥
3. ä¸ç·¨é€ ç­”æ¡ˆ
```

âŒ Bad:
```
è«‹æ ¹æ“šä¸Šä¸‹æ–‡å›ç­”ï¼Œå¦‚æœä¸çŸ¥é“å°±èªªä¸çŸ¥é“ã€‚
```

---

### 2. Explicit Constraints

âœ… Good:
```
**é‡è¦ç´„æŸï¼šä½ å¿…é ˆåƒ…ä½¿ç”¨ä¸‹æ–¹æä¾›çš„ã€Œä¸Šä¸‹æ–‡ã€ä¾†å›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚**

å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²’æœ‰ä¿¡æ¯ï¼Œå¿…é ˆå›æ‡‰ï¼š
"æ ¹æ“šæ‚¨æä¾›çš„æ–‡æª”ï¼Œæˆ‘ç„¡æ³•æ‰¾åˆ°ç›¸é—œä¿¡æ¯ã€‚"
```

âŒ Bad:
```
è«‹æ ¹æ“šä¸Šä¸‹æ–‡å›ç­”å•é¡Œã€‚
```

---

### 3. Format Specifications

âœ… Good (for Query Expansion):
```python
è«‹ä»¥ JSON æ ¼å¼å›æ‡‰ï¼š
{{
  "original_query": "{original_query}",
  "intent": "æŸ¥è©¢æ„åœ–æè¿°",
  "expanded_questions": ["å•é¡Œ1", "å•é¡Œ2"],
  "reasoning": "åˆ†è§£é‚è¼¯"
}}
```

âŒ Bad:
```
è«‹åˆ†è§£å•é¡Œä¸¦å›ç­”ã€‚
```

---

### 4. Examples in Prompts

Consider adding few-shot examples:

```python
SYSTEM_PROMPT_TEMPLATE = """ä½ æ˜¯ä¸€ä½æ–‡æª”å•ç­”åŠ©æ‰‹ã€‚

**å›ç­”ç¯„ä¾‹**ï¼š

ç”¨æˆ¶ï¼šã€Œé€™å€‹ç”¢å“çš„åƒ¹æ ¼æ˜¯å¤šå°‘ï¼Ÿã€
ä¸Šä¸‹æ–‡ï¼šã€Œæˆ‘å€‘çš„ç”¢å“å”®åƒ¹ç‚º NT$1,000ã€
âœ… æ­£ç¢ºå›ç­”ï¼šã€Œæ ¹æ“šæ–‡æª”ï¼Œé€™å€‹ç”¢å“çš„å”®åƒ¹ç‚º NT$1,000ã€‚ã€

ç”¨æˆ¶ï¼šã€Œé€™å€‹ç”¢å“æœ‰æŠ˜æ‰£å—ï¼Ÿã€
ä¸Šä¸‹æ–‡ï¼šã€Œæˆ‘å€‘çš„ç”¢å“å”®åƒ¹ç‚º NT$1,000ã€
âœ… æ­£ç¢ºå›ç­”ï¼šã€Œæ ¹æ“šæ–‡æª”å…§å®¹ï¼Œæ²’æœ‰æåŠæŠ˜æ‰£ä¿¡æ¯ã€‚ã€

---
[ä¸Šä¸‹æ–‡]
{context}
---

è«‹æ ¹æ“šä»¥ä¸Šä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ¶çš„å•é¡Œã€‚"""
```

---

## ğŸ”’ Important Notes

### Security Considerations

1. **Avoid Prompt Injection**: Don't allow user input directly into prompts without sanitization
2. **Context Length Limits**: Monitor total prompt length (system + context + query)
3. **PII Protection**: Don't log full prompts if they contain sensitive data

### Performance Considerations

1. **Prompt Length**: Longer prompts = higher latency and cost
2. **Token Limits**: Respect LLM context window (e.g., 4K, 8K, 32K tokens)
3. **Caching**: Consider caching expanded queries for similar questions

---

## ğŸ“š Additional Resources

- **Prompt Engineering Guide**: https://www.promptingguide.ai/
- **LangChain Prompt Templates**: https://python.langchain.com/docs/modules/model_io/prompts/
- **OpenAI Best Practices**: https://platform.openai.com/docs/guides/prompt-engineering

---

## ğŸ†˜ Troubleshooting

### Problem: Prompt changes not taking effect

**Solution**: 
1. Check server auto-reload logs
2. Manually restart server
3. Clear any prompt caching

### Problem: AI not following constraints

**Solution**:
1. Make constraints more explicit
2. Use stronger language ("MUST", "NEVER")
3. Add negative examples
4. Increase constraint repetition

### Problem: Query expansion too aggressive/conservative

**Solution**:
1. Adjust `EXPANSION_COUNT` in .env
2. Modify expansion prompt instructions
3. Change temperature setting

---

**End of Prompt Customization Guide**

For more help, see:
- `app/Services/prompt_service.py` - Source code with comments
- `app/Services/query_enhancement_service.py` - Query expansion implementation
- `claudedocs/core_logic_service_to_endpoint_chat.md` - Architecture explanation
