System Role: You are an experienced and profound architect
implementation third library: langchain, langgraph, 
task:

1. analyze the uploaded md file:ai_chat_v0.2.txt
2. the architecture contains three layers:
	- Foundmental Layer
	- Services Layer
	- Presentation Layer
3. the project structure is as following:
	app/
	 ├─ presentation/           # HTTP streaming, UIs, adapters for OPMP & exports
	 ├─ services/               # business/application services (pure python where possible)
	 ├─ llm_providers/          # provider adapters (OpenAI-compatible)
	 ├─ retrieval_providers/    # Milvus/FAISS/DuckDB adapters (optional, via interfaces)
	 ├─ domain/                 # Pydantic models, state enums, DTOs, errors
	 ├─ infra/                  # logging, config, caching, queues
	 └─ main.py                 # FastAPI/ASGI entry
4. This architecture must be a state-driven web application. StateTransitionService is primaily
   used in CoreLogicService	
5. Currently, the detailed files of Services Layer are as following:
	#### Domain models & state
		- domain/models.py
	#### CoreLogicService
		- services/core_logic.py
	#### InputDataHandleService
		- services/input_data_service.py
	#### PromptService
		- services/prompt_service.py
	#### StateTransitionService
		- services/state_transition_service.py
	#### LLMGateway
		- services/llm_gateway.py
6. Presentation Layer
	- OPMP(Optimistic Progressive Markdown Parsing)
	  please see the following uploaded files:
	  	*OPMP_Phase_Flow_Analysis.md
	- Data Export
		*Export PDF, Markdwon and other file formats

7. Request Flow (Sequence)
	```mermaid
sequenceDiagram
  participant UI as Presentation (UI/HTTP)
  participant CLS as CoreLogicService
  participant IDS as InputDataHandleService
  participant RS as RetrievalService
  participant PS as PromptService
  participant LG as LLMGateway
  participant LP as LLM Provider

  UI->>CLS: turn(query)
  CLS->>IDS: normalize(query)
  IDS-->>CLS: normalized input
  CLS->>RS: retrieve(input)
  RS-->>CLS: Evidence(chunks,citations)
  CLS->>PS: build(input, evidence)
  PS-->>CLS: PromptBundle(system,user,context)
  CLS->>LG: generate(bundle)
  LG->>LP: chat/stream (OpenAI-compatible)
  LP-->>LG: tokens/usage
  LG-->>CLS: markdown, usage
  CLS-->>UI: stream tokens
  UI->>UI: OPMP partial parse & render
  UI->>UI: Export on demand (md/json/csv/pdf)
```
8. FastAPI wiring (minimal)
```python
# main.py
from fastapi import FastAPI
from llm_providers.openai_compat import OpenAICompatClient
from services.llm_gateway import LLMGateway
from services.core_logic import CoreLogicService
from services.prompt_service import PromptService
from services.retrieval_service import RetrievalService
from services.input_data_service import InputDataHandleService
from services.state_transition_service import StateTransitionService
from presentation.http_stream import router as stream_router

def build_app():
    app = FastAPI()
    # Choose one provider at deploy-time:
    client = OpenAICompatClient(base_url="http://localhost:11434")   # Ollama
    llm = LLMGateway(client, default_model="llama3:8b-instruct")
    retriever = RetrievalService(store=YourVectorStore())             # impl elsewhere

    svc = CoreLogicService(
        prompt=PromptService(),
        retriever=retriever,
        state=StateTransitionService(),
        input_svc=InputDataHandleService(),
        llm=llm
    )
    stream_router.svc = svc
    app.include_router(stream_router, prefix="/api")
    return app

app = build_app()
```
